{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b1929f0",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from load_docs import load_documents,chunk_documents\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.llms import HuggingFaceHub\n",
    "import os\n",
    "import streamlit as st\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_community.llms import HuggingFaceEndpoint\n",
    "from transformers import pipeline\n",
    "from langchain_community.llms import HuggingFacePipeline\n",
    "from langchain.chains import ConversationalRetrievalChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a1ebe27",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# 1. Load PDF\n",
    "documents = load_documents(\"docs/na_tesla_service_plan_agreement_ot.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61600585",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# 2. Split into chunks\n",
    "chunks = chunk_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "116b124d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "embeddings = HuggingFaceEmbeddings(model_name=\"BAAI/bge-base-en\")\n",
    "vectorstore = Chroma.from_documents(chunks, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48bb868c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "hf_pipeline = pipeline(\n",
    "    \"text2text-generation\",\n",
    "    model=\"google/flan-t5-base\",\n",
    "    device=-1   # CPU, or 0 for GPU\n",
    ")\n",
    "\n",
    "llm = HuggingFacePipeline(pipeline=hf_pipeline)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec5617aa",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# 4. Create QA chain\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3})\n",
    "# Point to your deployed Inference Endpoint\n",
    "qa_chain = ConversationalRetrievalChain.from_llm(\n",
    "    llm=llm,\n",
    "    retriever=retriever\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d50d2415",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
